{
  "cells": [
    {
      "metadata": {
        "_uuid": "44b957bd51246c48aa937bd4abda40fcde50b2eb"
      },
      "cell_type": "markdown",
      "source": "## Before Starting:\nThanks NathanGeorge.(https://github.com/nateGeorge/preprocess_lending_club_data) for combining all smaller files togather. Here I started my data processing with this combined csv.\n\nThank Maxime C. Cohen, C. Daniel Guetta, Kevin Jiao, and Foster Provost for their case study article and code."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "# Import our libraries we are going to use for our data analysis.\nimport tensorflow as tf\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Plotly visualizations\nfrom plotly import tools\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nfrom scipy.stats import kendalltau, spearmanr\n\n# For oversampling Library (Dealing with Imbalanced Datasets)\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# Other Libraries\nimport time\n# Load sklearn utilities\n# ----------------------\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, brier_score_loss, mean_squared_error, r2_score\n\nfrom sklearn.calibration import calibration_curve\n\n\n% matplotlib inline\n\n\n# Load classifiers\n# ----------------\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\n# Other Packages\n# --------------\nfrom scipy.stats import kendalltau\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cluster import KMeans\n!pip install gurobipy\nfrom gurobipy import *\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\n!pip install pydotplus\nimport pydotplus\nfrom scipy.interpolate import spline\n\n# Load debugger, if required\n#import pixiedust\npd.options.mode.chained_assignment = None #'warn'\n\n# suppress all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv(\"../input/lending-club/accepted_2007_to_2018Q3.csv\",error_bad_lines=False)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "264e8497d3a66d774f3ed825a4dc060768af0768"
      },
      "cell_type": "markdown",
      "source": "\n# Data visualization and exploration\nFirst, we have a look at important features.\nWe will start by exploring the distribution of the loan amounts, funded_amounts and interest rates."
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "793b6f53f2f8fba88efa3892e189e4527e337d73"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(1, 3, figsize=(16,5))\n\nloan_amount = df[\"loan_amnt\"].dropna().values\nfunded_amount = df[\"funded_amnt\"].dropna().values\ninvestor_funds = df[\"int_rate\"].dropna().values\n\n\nsns.distplot(loan_amount, ax=ax[0], color=\"#F7522F\")\nax[0].set_title(\"Loan Applied by the Borrower\", fontsize=14)\nsns.distplot(funded_amount, ax=ax[1], color=\"#2F8FF7\")\nax[1].set_title(\"Amount Funded by the Lender\", fontsize=14)\nsns.distplot(investor_funds, ax=ax[2], color=\"#2EAD46\")\nax[2].set_title(\"Total committed by Investors\", fontsize=14)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2494e9a9b53582b2eee6a100f0adc62e96386352"
      },
      "cell_type": "markdown",
      "source": "Let's have a look at the distirubtion of the interest rates and the distribution of the annual income (excluding incomes above 200,000)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4191af4bfc9eeb1c16a428de47ecc757c169786"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(1, 2, figsize=(16,5))\n(df['int_rate']/len(df)).plot.hist(bins=10,ax=ax[0], color=\"#F7522F\",fontsize=8)\nax[0].set_title(\"Distribution of the Interest Rates\", fontsize=14)\ndf[df['annual_inc']<200000]['annual_inc'].plot.hist(bins=20,ax=ax[1], color=\"#2F8FF7\")\nax[1].set_title(\"Distribution of the Annual Income\", fontsize=14)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a396427e63e728c2f62692bb2019e3b97b58dc39"
      },
      "cell_type": "markdown",
      "source": "Let's have a look at the distribution of the annual income (excluding incomes above 200,000). This seems so be log-normal-distributed so we do a log-transform"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee58ea8d0cb1fcc41f664d3f78a3641d4b26dc17"
      },
      "cell_type": "code",
      "source": "df['annual_inc_log'] = df['annual_inc'].apply(np.log)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9442dd5120ca21a37840f004aceb3d4734327a0"
      },
      "cell_type": "code",
      "source": "(df['purpose'].value_counts()/len(df)).plot.bar()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7c986f169b77f4f169cfb988a82e77efd183c38"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(1, 2, figsize=(16,5))\ndf['fico_range_high'].plot.hist(bins=20, title='FICO-Score',ax=ax[0],color=\"#F7522F\")\ndf['installment'].plot.hist(bins=40, title='installment',ax=ax[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1316dadae7a1a9f7dfc6c908ebede28239ecfec4"
      },
      "cell_type": "markdown",
      "source": "Let's have a look at the distribution of the FICO score and the installment.\n\nFICO-Score:The upper boundary range the borrowerâ€™s FICO at loan origination belongs to.\n\nInstallment: The monthly payment owed by the borrower if the loan originates."
    },
    {
      "metadata": {
        "_uuid": "71afdf286a449914c7cefe717c75303cdfc7db73"
      },
      "cell_type": "markdown",
      "source": "# Model with leakage"
    },
    {
      "metadata": {
        "_uuid": "c69bfa801ae9e9df09c828be61d338602136e2c2"
      },
      "cell_type": "markdown",
      "source": "We have too many features and we cannot use all of them since some information cannot be gotten when predicting new data.  We might be tempted to use every attribute from the data in their models. Unfortunately, this would be ill-advised, because LendingClub updates some of these variables as time goes by. Thus, many of the variables in the data table will contain data that were not available at the time the loan was issued and therefore should not be used to create an investment strategy.\n\n**Here we have the issure of leakage. **\n\nIf any other feature whose value would not actually be available in practice at the time youâ€™d want to use the model to make a prediction, is a feature that can introduce leakage to your modelâ€” Data Skeptic\n\nAs discussed above, one common pitfall that arises in building and assessing predictive models is leakage from a situation in which the value of the target variable is known, back to the setting in which we evaluate the modelâ€”where we are pretending that the target variable is not known. This leakage can occur, for example, through another variable correlated with the target variable. Note that leakage is insidious because it often affects both the training and test data, and so, typical evaluations will give overly optimistic results.\n\n**It is interesting to note that the interest_rate variable does change occasionally after the loan is issued**. We will eventually get rid of this variable in building our models, so this should not be too worrying, but it is worth noting.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd6b0e248ba083873cecfabf6fe82800b68b3503"
      },
      "cell_type": "code",
      "source": "data = df[['id','loan_amnt','funded_amnt','funded_amnt_inv','term','int_rate',\n         'installment','grade','sub_grade','emp_title','emp_length',\n         'home_ownership','annual_inc','verification_status','issue_d',\n         'loan_status','purpose','title','zip_code','addr_state','dti','total_pymnt',\n         'delinq_2yrs','earliest_cr_line','open_acc','pub_rec','last_pymnt_d',\n         'last_pymnt_amnt','fico_range_high','fico_range_low','last_fico_range_high',\n         'last_fico_range_low','application_type','revol_bal','revol_util','recoveries']]\n\ndata.dropna(subset=['annual_inc','loan_status','issue_d','last_pymnt_d','loan_amnt',\n                    'int_rate','earliest_cr_line','open_acc','pub_rec','delinq_2yrs','recoveries',\n                    'grade','fico_range_high','fico_range_low','installment', 'last_fico_range_high',\n                    'last_fico_range_low','funded_amnt','dti','funded_amnt_inv','revol_bal','revol_util']\n            ,inplace=True)\ndata.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3145c7d186d424265ff048656e0c71e794ee2ac8"
      },
      "cell_type": "markdown",
      "source": "Identify the type of each of these column"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b33a5193cf3e30f41099fcdf9846fc028e5a0126"
      },
      "cell_type": "code",
      "source": "float_cols = ['loan_amnt', 'funded_amnt', 'installment', 'annual_inc',\n            'dti', 'revol_bal', 'delinq_2yrs', 'open_acc', 'pub_rec',\n                'fico_range_high', 'fico_range_low','last_fico_range_low',\n              'last_fico_range_high','total_pymnt', 'recoveries']\ncat_cols = ['term', 'grade', 'emp_length', 'home_ownership',\n                    'verification_status', 'loan_status', 'purpose']\nperc_cols = ['int_rate', 'revol_util']\ndate_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d']\n\nfor j in float_cols:\n    data[j] = pd.to_numeric(data[j])\n    \nfor j in perc_cols:\n    data[j] = data[j].astype(str).str.strip('%')\n    data[j] = pd.to_numeric(data[j])\n    data[j] = data[j]/100\n\nfor j in date_cols:\n    data[j] = pd.to_datetime(data[j])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b2e14a82f912c0ab00a654e6a179451ea3f2f16a"
      },
      "cell_type": "markdown",
      "source": "Engineer the features and generate the training/testing set"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d658766b9ce9c5dfe2cc9cf75dac50ccc5d8369"
      },
      "cell_type": "code",
      "source": "default_seed = 1\nnp.random.seed(default_seed)\n\n# make sure selecting only terminated loans\n#data = data[data.loan_status.isin(['Fully Paid','Charged Off','Default'])]\n\n# downsample\ndata = data.sample(n=50000)\n\n# create labels for the dataset\ndata['label'] = (data.loan_status.str.contains('Charged Off') | \n                data.loan_status.str.contains('Default'))\ndata['cr_hist'] = (data.issue_d - data.earliest_cr_line) / np.timedelta64(1, 'M')\ndata.label = data.label.astype(int)\n\n\n\n# clean and get training/testing data \ntemp = pd.get_dummies(data[['term','grade','emp_length','home_ownership',\n                                  'verification_status','purpose']],dummy_na=True)\n\nX = data.as_matrix(columns=['loan_amnt','funded_amnt','int_rate','installment',\n                            'annual_inc','dti','delinq_2yrs','open_acc','pub_rec',\n                            'fico_range_high','fico_range_low','cr_hist','revol_bal',\n                            'recoveries','last_fico_range_high','last_fico_range_low',\n                            'revol_util', 'total_pymnt'])\n\nX = np.concatenate((X,temp.as_matrix()),axis=1)\ny = data.label.as_matrix()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train = min_max_scaler.fit_transform(X_train)\nX_test = min_max_scaler.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3cd38df79c2ce2686ffe7f2c6ef9a9c2c90c0f9f"
      },
      "cell_type": "markdown",
      "source": "### Classification models\n#### ð‘™2 penalized logistic regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94de24474ef39517bd3ea66851c312e77cda1b59",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "logisticModel = LogisticRegressionCV(cv=10,penalty='l2')\nlogisticModel.fit(X_train,y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3a3417b54bae310f891914cd1a6aa0b8deef8c7"
      },
      "cell_type": "code",
      "source": "y_pred = logisticModel.predict(X_test)\nprint('accuracy: ',accuracy_score(y_test,y_pred))\ntarget_names = ['Non-Defaulted Loan','Defaulted Loan']\nprint(classification_report(y_test,y_pred,target_names=target_names,digits=4))\nprint('AUC: ',roc_auc_score(y_test,logisticModel.predict_proba(X_test)[:,1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "029cda2d0289e36ba592e3a9c8427b5053d3d822"
      },
      "cell_type": "code",
      "source": "fpr, tpr, thresholds = roc_curve(y_test, logisticModel.predict_proba(X_test)[:,1],\n                                         pos_label=1)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',\nlabel='AUC = %0.2f'% roc_auc_score(y_test,logisticModel.predict_proba(X_test)[:,1]))\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90e1d5412def1b1813c21f9507bbef5d3579e832"
      },
      "cell_type": "markdown",
      "source": "### Random forest\nIn addition to logistic regression, let's also take a quick look at random forest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb336565e975abb99d34c712886230ecbabee271"
      },
      "cell_type": "code",
      "source": "random_forest = RandomForestClassifier(min_samples_leaf=100,n_estimators=50)\nrandom_forest.fit(X_train,y_train)\ny_pred = random_forest.predict(X_test)\nprint('accuracy: ',accuracy_score(y_test,y_pred))\ntarget_names = ['Non-Defaulted Loan','Defaulted Loan']\nprint(classification_report(y_test,y_pred,target_names=target_names,digits=4))\nprint('AUC: ',roc_auc_score(y_test,random_forest.predict_proba(X_test)[:,1]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b1923aa7610e4791e88eb86d8546659041574088"
      },
      "cell_type": "markdown",
      "source": "As we can see, with leaked features the AUC of the models is ridiculously high."
    },
    {
      "metadata": {
        "_uuid": "06710dc0fd75c7114f550a858883bb421ef9287e"
      },
      "cell_type": "markdown",
      "source": "# Model without leakage"
    },
    {
      "metadata": {
        "_uuid": "f3733047618ad93155f5f05198d9947b20d160cd"
      },
      "cell_type": "markdown",
      "source": "## Data processing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "777e0608c774f32447346f4bd9042bdb6483d3ae"
      },
      "cell_type": "code",
      "source": "# Identify the columns we'll be keeping from the dataset\ncols_to_pick = ['id','loan_amnt','funded_amnt','term','int_rate',\n                 'installment','grade','emp_length', 'home_ownership',\n                 'annual_inc','verification_status','issue_d',\n                 'loan_status','purpose','dti', 'delinq_2yrs',\n                 'earliest_cr_line','open_acc','pub_rec', 'fico_range_high',\n                 'fico_range_low', 'revol_bal','revol_util', 'total_pymnt',\n                                                    'last_pymnt_d', 'recoveries']\n\n# Identify the type of each of these column\nfloat_cols = ['loan_amnt', 'funded_amnt', 'installment', 'annual_inc',\n                     'dti', 'revol_bal', 'delinq_2yrs', 'open_acc', 'pub_rec',\n                                'fico_range_high', 'fico_range_low', 'total_pymnt', 'recoveries']\ncat_cols = ['term', 'grade', 'emp_length', 'home_ownership',\n                    'verification_status', 'loan_status', 'purpose']\nperc_cols = ['int_rate', 'revol_util']\ndate_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d']\n\n# Ensure that we have types for every column\nassert set(cols_to_pick) - set(float_cols) - set(cat_cols) - set(perc_cols) - set(date_cols) == set([\"id\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d707d9a3b9e0705d96f069a64eecaf3d920dcf9f"
      },
      "cell_type": "code",
      "source": "final_data= df[cols_to_pick].copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de9c04d1e09ba26bd10ea4e0748a4db5ef3912b9"
      },
      "cell_type": "markdown",
      "source": "Typecast the columns"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29025cdc1dc81bcea627304000fc3f5cb1fdcdaf"
      },
      "cell_type": "code",
      "source": "for i in float_cols:\n    final_data[i] = final_data[i].astype(float)\n\ndef clean_perc(x):\n    if pd.isnull(x):\n        return np.nan\n    else:\n        return float(x.strip()[:-1])\nfor j in perc_cols:\n    final_data[j] = final_data[j].astype(str).str.strip('%')\n    #final_data[j] = pd.to_numeric(data[j])\n    #final_data[j] = (final_data[j]/100).apply( clean_perc )\n    \ndef clean_date(x):\n    if pd.isnull(x):\n        return None\n    else:\n        return datetime.datetime.strptime( x, \"%b-%Y\").date()\nfor i in date_cols:\n    final_data[i] = final_data[i].apply( clean_date )\n    \nfor i in cat_cols:\n    final_data.loc[final_data[i].isnull(), i] = None",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eed31388252d1de0e29ec1c6cbe2c36d83beed20"
      },
      "cell_type": "markdown",
      "source": "### Calculate returns for each loan\n\nThis question appears simple. But in reality, it is not. There are two factors making things complicated: \n1. the return should take into account defaulted loans, which usually are partially paid off, and \n2. the return should also take into account loans that have been paid early (i.e., before the loan term is completed).\nWe want to make it easy and there are 3 methods.\n\nMethod 1 ( M1â€”Pessimistic) supposes that, once the loan is paid back, the investor is forced to sit with the money without rein- vesting it anywhere else until the term of the loan.\n\nMethod 2 (M2â€”Optimistic) supposes that, once the loan is paid back, the investorâ€™s money is returned and the investor can im- mediately invest in another loan with exactly the same return.\n\nMethod 3 ( M3) considers a fixed time hori- zon (e.g., T months) and calculates the return on investing in a particular loan under the assumption that any revenues paid out from the loan are immediately rein- vested at a yearly rate of i%, compounded monthly, until the T-month horizon is over (throughout this case study, we consider a 5-year horizon, i.e., T = 60)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47c57248bfb984eafbebec88eab1b9ed14b0e10a"
      },
      "cell_type": "code",
      "source": "# Define the names of the four returns we'll be calculating\nret_cols = [\"ret_PESS\", \"ret_OPT\", \"ret_INTa\", \"ret_INTb\", \"ret_INTc\"]\n# Remove all rows for loans that were paid back on the days they were issued\nfinal_data['loan_length'] = (final_data.last_pymnt_d - final_data.issue_d) / np.timedelta64(1, 'M')\nn_rows = len(final_data)\nfinal_data = final_data[final_data.loan_length != 0]\nprint(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "65123bf05d00cc1c6663e00e0a4577a60c99e789"
      },
      "cell_type": "markdown",
      "source": "Return Method 2 (pessimistic)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd75c477b66e12fe1b2bae5c85d66250f09d52b0"
      },
      "cell_type": "code",
      "source": "final_data = final_data[pd.notnull(df['term'])]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22ecfa96c06ce65cd8d67eaacce82ce0c41889ad"
      },
      "cell_type": "code",
      "source": "# Calculate the return using a simple annualized profit margin\n# Pessimistic fefinition (method 2)\n\nfinal_data['term_num'] = final_data.term.str.extract('(\\d+)',expand=False).astype(int)\nfinal_data['ret_PESS'] = ( (final_data.total_pymnt - final_data.funded_amnt) \n                                            / final_data.funded_amnt ) * (12 / final_data['term_num'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c07608a91910c630977c474ab600da0f26bcf251"
      },
      "cell_type": "markdown",
      "source": "Return Method 1 (optimistic)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8870b7d423e79126f6247af258fd77425699e0c"
      },
      "cell_type": "code",
      "source": "# Assuming that if a loan gives a positive return, we can\n# immediately find a similar loan to invest in; if the loan\n# takes a loss, we use method 2 to compute the return\n\nfinal_data['ret_OPT'] = ( (final_data.total_pymnt - final_data.funded_amnt)\n                                            / final_data.funded_amnt ) * (12 / final_data['loan_length'])\nfinal_data.loc[final_data.ret_OPT < 0,'ret_OPT'] = final_data.ret_PESS[final_data.ret_OPT < 0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "29ea8e592ca8773ea54d5a496e3cc35038961b56"
      },
      "cell_type": "markdown",
      "source": "Return Method 3 (re-investment)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb89ebb65b41b3a3b1ce2c8124c492346af935f9"
      },
      "cell_type": "code",
      "source": "def ret_method_3(T, i):\n    '''\n    Given an investment time horizon (in months) and re-investment\n    interest rate, calculate the return of each loan\n    '''\n    \n    # Assuming that the total amount paid back was paid at equal\n    # intervals during the duration of the loan, calculate the\n    # size of each of these installment\n    actual_installment = (final_data.total_pymnt - final_data.recoveries) / final_data['loan_length']\n\n    # Assuming the amount is immediately re-invested at the prime\n    # rate, find the total amount of money we'll have by the end\n    # of the loan\n    cash_by_end_of_loan = actual_installment * (1 - pow(1 + i, final_data.loan_length)) / ( 1 - (1 + i) )\n    \n    cash_by_end_of_loan = cash_by_end_of_loan + final_data.recoveries\n    \n    # Assuming that cash is then re-invested at the prime rate,\n    # with monthly re-investment, until T months from the start\n    # of the loan\n    remaining_months = T - final_data['loan_length']\n    final_return = cash_by_end_of_loan * pow(1 + i, remaining_months)\n\n    # Find the percentage return\n    return( (12/T) * ( ( final_return - final_data['funded_amnt'] ) / final_data['funded_amnt'] ) )\n\nfinal_data['ret_INTa'] = ret_method_3(5*12, 0.001)\nfinal_data['ret_INTb'] = ret_method_3(5*12, 0.0025)\nfinal_data['ret_INTc'] = ret_method_3(5*12, 0.005)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1739fe399dce376c5057e77d371e34fe2ebdf64a"
      },
      "cell_type": "markdown",
      "source": "### Visualize the variables"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e2eddc9f0fef9c80382456c7865ac49bba1e320"
      },
      "cell_type": "code",
      "source": "def visualize_columns():\n    '''\n    This function visualizes all columns\n      - Box-and-whisker plots for continuous variables\n      - Lists of distinct values for categorical columns\n      - A timeline density for dates\n    '''\n    \n    # FLoat columns\n    for i in float_cols + perc_cols + ret_cols:\n        seaborn.boxplot(final_data[i])\n\n        # Print the three highest values\n        highest_vals = sorted(final_data[i], reverse=True)[:3]\n        smallest_val = min(final_data[i])\n        plt.text(smallest_val, -0.3, highest_vals[0])\n        plt.text(smallest_val, -0.2, highest_vals[1])\n        plt.text(smallest_val, -0.1, highest_vals[2])\n\n        plt.show()\n        \n    # Categorical columns \n    for i in cat_cols:\n        print(i)\n        print(str(len(set(final_data[i]))) + \" distinct values\")\n        print(final_data[i].value_counts())\n        print(\"\")\n        print(\"\")\n    \n    # Date columns\n    for i in date_cols:\n        final_data[final_data[i].isnull() == False][i].apply(lambda x : str(x.year) +\n                                                \"-\" + str(x.month)).value_counts(ascending = True).plot()\n        plt.title(i + \" (\" + str(final_data[i].isnull().sum()) + \" null values)\")\n        plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1a79dbad7346046f1bf8e45b30cebbbe2971ae7"
      },
      "cell_type": "code",
      "source": "import seaborn\n#visualize_columns()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c9fa0eb21185a4eeba90ae994dcb7ecc65f05e0"
      },
      "cell_type": "markdown",
      "source": "### Handle outliers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1466242c29aefb292de072d20f6efd0766e227a4"
      },
      "cell_type": "code",
      "source": "# There are quite a few outliers, but the two most obvious\n# ones to remove are in annual_inc, revol_util Remove these.\nn_rows = len(final_data)\nfinal_data = final_data[final_data.annual_inc < 10999200]\nfinal_data = final_data[final_data.revol_util.astype(float) < 300]\nprint(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3906249ba0174c865ff4207fc64957b4100b9d86"
      },
      "cell_type": "code",
      "source": "# Remove all loans that are too recent to have been paid off or\n# defaulted\nn_rows = len(final_data)\nfinal_data = final_data[final_data.loan_status.isin(['Fully Paid','Charged Off','Default'])]\nprint(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "373fc19c49f26158e801fc886ae5f11c10d44705"
      },
      "cell_type": "code",
      "source": "# Only include loans isssued since 2009\nn_rows = len(final_data)\nfinal_data = final_data[final_data.issue_d >= datetime.date(2009, 1, 1)]\nprint(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "341b62973a63234e7d2fbb9db45520912c9162f8"
      },
      "cell_type": "markdown",
      "source": "### Deal with null values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6987420859fdc422e25834780dc87fffa0675bb9"
      },
      "cell_type": "code",
      "source": "# Deal with null values. We allow cateogrical variables to be null\n# OTHER than grade, which is a particularly important categorical.\n# All non-categorical variables must be non-null, and we drop\n# rows that do not meet this requirement\nrequired_cols = set(cols_to_pick) - set(cat_cols) - set([\"id\"])\nrequired_cols.add(\"grade\")\n\nn_rows = len(final_data)\nfinal_data.dropna(subset = required_cols ,inplace=True)\nprint(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b190d57c106fb0368d5006228d1a047f5cd1a24"
      },
      "cell_type": "code",
      "source": "# Create the outcome\ndata=final_data.copy()\ndata[\"outcome\"] = data.loan_status.isin([\"Charged Off\", \"Default\"])\ndata.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aef837e30767be3698870d6def57a3c142d90e37"
      },
      "cell_type": "code",
      "source": "discrete_features = list(set(cat_cols) - set([\"loan_status\"]))\n\n# All numeric columns will be used as continuous features\ncontinuous_features = list(float_cols + perc_cols)\ncontinuous_features = [i for i in continuous_features if i not in [\"total_pymnt\", \"recoveries\"]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3a7259c5c1fbe95f46be8c73b9aa043b9f2fc06"
      },
      "cell_type": "code",
      "source": "# Create a feature for the length of a person's credit history at the\n# time the loan is issued\ndata['cr_hist'] = (data.issue_d - data.earliest_cr_line) / np.timedelta64(1, 'M')\ncontinuous_features.append('cr_hist')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec9e847480100236b4e18d7fe28355cae76492f0"
      },
      "cell_type": "code",
      "source": "# Randomly assign each row to a training and test set. We do this now\n# because we will be fitting a variety of models on various time periods,\n# and we would like every period to use the *same* training/test split\nnp.random.seed(default_seed)\ndata['train'] = np.random.choice([True, False], size = len(data), p = [0.7, 0.3])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f56a7bf6be64011f83d97e368bd7ae93c9be8bd1"
      },
      "cell_type": "code",
      "source": "# Create a matrix of features and outcomes, with dummies. Record the\n# names of the dummies for later use\nX_continuous = data[continuous_features].values\n\nX_discrete = pd.get_dummies(data[discrete_features], dummy_na = True, prefix_sep = \"::\", drop_first = True)\ndiscrete_features_dummies = X_discrete.columns.tolist()\nX_discrete = X_discrete.values\n\nX = np.concatenate( (X_continuous, X_discrete), axis = 1 )\n\ny = data.outcome.values\n\ntrain = data.train.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "182adb00f6005178d43c6b4ee9d2762292bee921"
      },
      "cell_type": "markdown",
      "source": "## Prepare functions to fit and evaluate models"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e05c8166689dbb51cfb589c625f931d7b5edb146"
      },
      "cell_type": "code",
      "source": "def prepare_data(data_subset = np.array([True]*len(data)),\n                    n_samples_train = 30000,\n                    n_samples_test = 20000,\n                    feature_subset = None,\n                    date_range_train = (data.issue_d.min(), data.issue_d.max()),\n                    date_range_test = (data.issue_d.min(), data.issue_d.max()),\n                    random_state = default_seed):\n    '''\n    This function will prepare the data for classification or regression.\n    It expects the following parameters:\n      - data_subset: a numpy array with as many entries as rows in the\n                     dataset. Each entry should be True if that row\n                     should be used, or False if it should be ignored\n      - n_samples_train: the total number of samples to be used for training.\n                         Will trigger an error if this number is larger than\n                         the number of rows available after all filters have\n                         been applied\n      - n_samples_test: as above for testing\n      - feature_subect: A list containing the names of the features to be\n                        used in the model. In None, all features in X are\n                        used\n      - date_range_train: a tuple containing two dates. All rows with loans\n                          issued outside of these two dates will be ignored in\n                          training\n      - date_range_test: as above for testing\n      - random_state: the random seed to use when selecting a subset of rows\n      \n    Note that this function assumes the data has a \"Train\" column, and will\n    select all training rows from the rows with \"True\" in that column, and all\n    the testing rows from those with a \"False\" in that column.\n    \n    This function returns a dictionary with the following entries\n      - X_train: the matrix of training data\n      - y_train: the array of training labels\n      - train_set: a Boolean vector with as many entries as rows in the data\n                  that denotes the rows that were used in the train set\n      - X_test: the matrix of testing data\n      - y_test: the array of testing labels\n      - test_set: a Boolean vector with as many entries as rows in the data\n                  that denotes the rows that were used in the test set\n    '''\n    \n    np.random.seed(random_state)\n        \n    # Filter down the data to the required date range, and downsample\n    # as required\n    filter_train = ( train & (data.issue_d >= date_range_train[0]) &\n                            (data.issue_d <= date_range_train[1]) & data_subset ).values\n    filter_test = ( (train == False) & (data.issue_d >= date_range_test[0])\n                            & (data.issue_d <= date_range_test[1]) & data_subset ).values\n    \n    filter_train[ np.random.choice( np.where(filter_train)[0], size = filter_train.sum()\n                                                   - n_samples_train, replace = False ) ] = False\n    filter_test[ np.random.choice( np.where(filter_test)[0], size = filter_test.sum()\n                                                   - n_samples_test, replace = False ) ] = False\n    \n    # Prepare the training and test set\n    X_train = X[ filter_train , :]\n    X_test = X[ filter_test, :]\n    if feature_subset != None:\n        cols = [i for i, j in enumerate(continuous_features + discrete_features_dummies)\n                                                     if j.split(\"::\")[0] in feature_subset]\n        X_train = X_train[ : , cols ]\n        X_test = X_test[ : , cols ]\n        \n    y_train = y[ filter_train ]\n    y_test = y[ filter_test ]\n    \n    # Scale the variables\n    scaler = preprocessing.MinMaxScaler()\n\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    # return training and testing data\n    out = {'X_train':X_train, 'y_train':y_train, 'train_set':filter_train, \n           'X_test':X_test, 'y_test':y_test, 'test_set':filter_test}\n    \n    return out",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f5c67f8dbd963d6d34c8adaddfb4a69e0bb3934"
      },
      "cell_type": "code",
      "source": "def fit_classification(model, data_dict,\n                          cv_parameters = {},\n                          model_name = None,\n                          random_state = default_seed,\n                          output_to_file = True,\n                          print_to_screen = True):\n    '''\n    This function will fit a classification model to data and print various evaluation\n    measures. It expects the following parameters\n      - model: an sklearn model object\n      - data_dict: the dictionary containing both training and testing data;\n                   returned by the prepare_data function\n      - cv_parameters: a dictionary of parameters that should be optimized\n                       over using cross-validation. Specifically, each named\n                       entry in the dictionary should correspond to a parameter,\n                       and each element should be a list containing the values\n                       to optimize over\n      - model_name: the name of the model being fit, for printouts\n      - random_state: the random seed to use\n      - output_to_file: if the results will be saved to the output file\n      - print_to_screen: if the results will be printed on screen\n    \n    If the model provided does not have a predict_proba function, we will\n    simply print accuracy diagnostics and return.\n    \n    If the model provided does have a predict_proba function, we first\n    figure out the optimal threshold that maximizes the accuracy and\n    print out accuracy diagnostics. We then print an ROC curve, sensitivity/\n    specificity curve, and calibration curve.\n    \n    This function returns a dictionary with the following entries\n      - model: the best fitted model\n      - y_pred: predictions for the test set\n      - y_pred_probs: probability predictions for the test set, if the model\n                      supports them\n      - y_pred_score: prediction scores for the test set, if the model does not \n                      output probabilities.\n    '''\n        \n    np.random.seed(random_state)\n    \n    # --------------------------\n    #   Step 1 - Load the data\n    # --------------------------\n    X_train = data_dict['X_train']\n    y_train = data_dict['y_train']\n    \n    X_test = data_dict['X_test']\n    y_test = data_dict['y_test']\n    \n    filter_train = data_dict['train_set']    \n  \n    # --------------------------\n    #   Step 2 - Fit the model\n    # --------------------------\n\n    cv_model = GridSearchCV(model, cv_parameters)\n    \n    start_time = time.time()\n    cv_model.fit(X_train, y_train)\n    end_time = time.time()\n    \n    best_model = cv_model.best_estimator_\n    \n    if print_to_screen:\n\n        if model_name != None:\n            print(\"=========================================================\")\n            print(\"  Model: \" + model_name)\n            print(\"=========================================================\")\n\n        print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n        print(\"Optimal parameters:\")\n        print(cv_model.best_params_)\n        print(\"\")\n    \n    # -------------------------------\n    #   Step 3 - Evaluate the model\n    # -------------------------------\n    \n    # If possible, make probability predictions\n    try:\n        y_pred_probs = best_model.predict_proba(X_test)[:,1]\n        fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n        \n        probs_predicted = True\n    except:\n        probs_predicted = False\n    \n    # Make predictions; if we were able to find probabilities, use\n    # the threshold that maximizes the accuracy in the training set.\n    # If not, just use the learner's predict function\n    if probs_predicted:\n        y_train_pred_probs = best_model.predict_proba(X_train)[:,1]\n        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_pred_probs)\n        \n        true_pos_train = tpr_train*(y_train.sum())\n        true_neg_train = (1 - fpr_train) *(1-y_train).sum()\n        \n        best_threshold_index = np.argmax(true_pos_train + true_neg_train)\n        best_threshold = 1 if best_threshold_index == 0 else thresholds_train[ best_threshold_index ]\n        \n        if print_to_screen:\n            print(\"Accuracy-maximizing threshold was: \" + str(best_threshold))\n        \n        y_pred = (y_pred_probs > best_threshold)\n    else:\n        y_pred = best_model.predict(X_test)\n    \n    if print_to_screen:\n        print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n        print(classification_report(y_test, y_pred, target_names =['No default', 'Default'], digits = 4))\n\n    if print_to_screen:\n        if probs_predicted:        \n            plt.figure(figsize = (13, 4.5))\n            plt.subplot(2, 2, 1)\n\n            plt.title(\"ROC Curve (AUC = %0.2f)\"% roc_auc_score(y_test, y_pred_probs))\n            plt.plot(fpr, tpr, 'b')\n            plt.plot([0,1],[0,1],'r--')\n            plt.xlim([0,1]); plt.ylim([0,1])\n            plt.ylabel('True Positive Rate')\n            plt.xlabel('False Positive Rate')\n\n            plt.subplot(2, 2, 3)\n\n            plt.plot(thresholds, tpr, 'b', label = 'Sensitivity')\n            plt.plot(thresholds, 1 -fpr, 'r', label = 'Specificity')\n            plt.legend(loc = 'lower right')\n            plt.xlim([0,1]); plt.ylim([0,1])\n            plt.xlabel('Threshold')\n\n            plt.subplot(2, 2, 2)\n\n            fp_0, mpv_0 = calibration_curve(y_test, y_pred_probs, n_bins = 10)\n            plt.plot([0,1], [0,1], 'k:', label='Perfectly calibrated')\n            plt.plot(mpv_0, fp_0, 's-')\n            plt.ylabel('Fraction of Positives')\n            plt.xlim([0,1]); plt.ylim([0,1])\n            plt.legend(loc ='upper left')\n            \n            plt.subplot(2, 2, 4)\n            plt.hist(y_pred_probs, range=(0, 1), bins=10, histtype=\"step\", lw=2)\n            plt.xlim([0,1]); plt.ylim([0,20000])\n            plt.xlabel('Mean Predicted Probability')\n            plt.ylabel('Count')\n            \n            #plt.tight_layout()\n            plt.show()\n        \n    # Additional Score Check\n    if probs_predicted:\n        y_train_score = y_train_pred_probs\n    else:\n        y_train_score = best_model.decision_function(X_train)\n        \n    tau, p_value = kendalltau(y_train_score, data.grade[filter_train])\n    if print_to_screen:\n        print(\"\")\n        print(\"Similarity to LC grade ranking: \", tau)\n    \n    if probs_predicted:\n        brier_score = brier_score_loss(y_test, y_pred_probs)\n        if print_to_screen:\n            print(\"Brier score:\", brier_score)\n    \n    # Return the model predictions, and the\n    # test set\n    # -------------------------------------\n    out = {'model':best_model, 'y_pred_labels':y_pred}\n    \n    if probs_predicted:\n        out.update({'y_pred_probs':y_pred_probs})\n    else:\n        y_pred_score = best_model.decision_function(X_test)\n        out.update({'y_pred_score':y_pred_score})\n        \n    # Output results to file\n    # ----------------------\n    if probs_predicted and output_to_file:\n        # Check whether any of the CV parameters are on the edge of\n        # the search space\n        opt_params_on_edge = find_opt_params_on_edge(cv_model)\n        dump_to_output(model_name + \"::search_on_edge\", opt_params_on_edge)\n        if print_to_screen:\n            print(\"Were parameters on edge? : \" + str(opt_params_on_edge))\n        \n        # Find out how different the scores are for the different values\n        # tested for by cross-validation. If they're not too different, then\n        # even if the parameters are off the edge of the search grid, we should\n        # be ok\n        score_variation = find_score_variation(cv_model)\n        dump_to_output(model_name + \"::score_variation\", score_variation)\n        if print_to_screen:\n            print(\"Score variations around CV search grid : \" + str(score_variation))\n        \n        # Print out all the scores\n        dump_to_output(model_name + \"::all_cv_scores\", str(cv_model.cv_results_['mean_test_score']))\n        if print_to_screen:\n            print( str(cv_model.cv_results_['mean_test_score']) )\n        \n        # Dump the AUC to file\n        dump_to_output(model_name + \"::roc_auc\", roc_auc_score(y_test, y_pred_probs) )\n        \n    return out",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f32485e517fb04847f95e3a6ff20642e59b918b"
      },
      "cell_type": "code",
      "source": "def fit_regression(model, data_dict,\n                      cv_parameters = {},\n                      separate = False, \n                      model_name = None,\n                      random_state = default_seed,\n                      output_to_file = True,\n                      print_to_screen = True):\n    '''\n    This function will fit a regression model to data and print various evaluation\n    measures. It expects the following parameters\n      - model: an sklearn model object\n      - data_dict: the dictionary containing both training and testing data;\n                   returned by the prepare_data function\n      - separate: a Boolean variable indicating whether we fit models for \n                  defaulted and non-defaulted loans separately\n      - cv_parameters: a dictionary of parameters that should be optimized\n                       over using cross-validation. Specifically, each named\n                       entry in the dictionary should correspond to a parameter,\n                       and each element should be a list containing the values\n                       to optimize over      \n      - model_name: the name of the model being fit, for printouts\n      - random_state: the random seed to use\n      - output_to_file: if the results will be saved to the output file\n      - print_to_screen: if the results will be printed on screen\n    \n    This function returns a dictionary FOR EACH RETURN DEFINITION with the following entries\n      - model: the best fitted model\n      - predicted_return: prediction result based on the test set\n      - predicted_regular_return: prediction result for non-defaulted loans (valid if separate == True)\n      - predicted_default_return: prediction result for defaulted loans (valid if separate == True)\n      - r2_scores: the testing r2_score(s) for the best fitted model\n    '''\n    \n    np.random.seed(random_state)\n    \n    # --------------------------\n    #   Step 1 - Load the data\n    # --------------------------\n    \n    col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n    \n    X_train = data_dict['X_train']\n    filter_train = data_dict['train_set']  \n\n    X_test = data_dict['X_test']\n    filter_test = data_dict['test_set']\n    out = {}\n    \n    for ret_col in col_list:\n        \n        y_train = data.loc[filter_train, ret_col].as_matrix()\n        y_test = data.loc[filter_test, ret_col].as_matrix() \n\n        # --------------------------\n        #   Step 2 - Fit the model\n        # --------------------------\n\n        if separate:\n            outcome_train = data.loc[filter_train, 'outcome']\n            outcome_test = data.loc[filter_test, 'outcome']\n\n            # Train two separate regressors for defaulted and non-defaulted loans\n            X_train_0 = X_train[outcome_train == False]\n            y_train_0 = y_train[outcome_train == False]\n            X_test_0 = X_test[outcome_test == False]\n            y_test_0 = y_test[outcome_test == False]\n\n            X_train_1 = X_train[outcome_train == True]\n            y_train_1 = y_train[outcome_train == True]\n            X_test_1 = X_test[outcome_test == True]\n            y_test_1 = y_test[outcome_test == True]\n\n            cv_model_0 = GridSearchCV(model, cv_parameters, scoring='r2')\n            cv_model_1 = GridSearchCV(model, cv_parameters, scoring='r2')\n\n            start_time = time.time()\n            cv_model_0.fit(X_train_0, y_train_0)\n            cv_model_1.fit(X_train_1, y_train_1)\n            end_time = time.time()\n\n            best_model_0 = cv_model_0.best_estimator_\n            best_model_1 = cv_model_1.best_estimator_\n            \n            if print_to_screen:\n\n                if model_name != None:\n                    print(\"=========================================================\")\n                    print(\"  Model: \" + model_name + \"  Return column: \" + ret_col)\n                    print(\"=========================================================\")\n\n                print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n                print(\"Optimal parameters:\")\n                print(\"model_0:\",cv_model_0.best_params_, \"model_1\",cv_model_1.best_params_)\n\n            predicted_regular_return = best_model_0.predict(X_test)\n            predicted_default_return = best_model_1.predict(X_test)\n            \n            if print_to_screen:\n                print(\"\")\n                print(\"Testing r2 scores:\")\n            # Here we use different testing set to report the performance\n            test_scores = {'model_0':r2_score(y_test_0,best_model_0.predict(X_test_0)),\n                              'model_1':r2_score(y_test_1,best_model_1.predict(X_test_1))}\n            if print_to_screen:\n                print(\"model_0:\", test_scores['model_0'])\n                print(\"model_1:\", test_scores['model_1'])\n\n            cv_objects = {'model_0':cv_model_0, 'model_1':cv_model_1}\n            out[ret_col] = { 'model_0':best_model_0, 'model_1':best_model_1, 'predicted_regular_return':predicted_regular_return,\n                      'predicted_default_return':predicted_default_return,'r2_scores':test_scores }\n\n        else:\n            cv_model = GridSearchCV(model, cv_parameters, scoring='r2')\n\n            start_time = time.time()\n            cv_model.fit(X_train, y_train)\n            end_time = time.time()\n\n            best_model = cv_model.best_estimator_\n            \n            if print_to_screen:\n                if model_name != None:\n                    print(\"=========================================================\")\n                    print(\"  Model: \" + model_name + \"  Return column: \" + ret_col)\n                    print(\"=========================================================\")\n\n                print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n                print(\"Optimal parameters:\")\n                print(cv_model.best_params_)\n\n            predicted_return = best_model.predict(X_test)\n            test_scores = {'model':r2_score(y_test,predicted_return)}\n            if print_to_screen:\n                print(\"\")\n                print(\"Testing r2 score:\", test_scores['model'])\n\n            cv_objects = {'model':cv_model}\n            out[ret_col] = {'model':best_model, 'predicted_return':predicted_return, 'r2_scores':r2_score(y_test,predicted_return)}\n\n        # Output the results to a file\n        if output_to_file:\n            for i in cv_objects:\n                # Check whether any of the CV parameters are on the edge of\n                # the search space\n                opt_params_on_edge = find_opt_params_on_edge(cv_objects[i])\n                dump_to_output(model_name + \"::\" + ret_col + \"::search_on_edge\", opt_params_on_edge)\n                if print_to_screen:\n                    print(\"Were parameters on edge (\" + i + \") : \" + str(opt_params_on_edge))\n\n                # Find out how different the scores are for the different values\n                # tested for by cross-validation. If they're not too different, then\n                # even if the parameters are off the edge of the search grid, we should\n                # be ok\n                score_variation = find_score_variation(cv_objects[i])\n                dump_to_output(model_name + \"::\" + ret_col + \"::score_variation\", score_variation)\n                if print_to_screen:\n                    print(\"Score variations around CV search grid (\" + i + \") : \" + str(score_variation))\n\n                # Print out all the scores\n                dump_to_output(model_name + \"::all_cv_scores\", str(cv_objects[i].cv_results_['mean_test_score']))\n                if print_to_screen:\n                    print(\"All test scores : \" + str(cv_objects[i].cv_results_['mean_test_score']) )\n\n                # Dump the AUC to file\n                dump_to_output( model_name + \"::\" + ret_col + \"::r2\", test_scores[i] )\n\n    return out",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfa2786c7881b2944c5f17d0577b923e21a33305"
      },
      "cell_type": "code",
      "source": "def test_investments(data_dict,\n                        classifier = None,\n                        regressor = None,\n                        strategy = 'Random', \n                        num_loans = 1000,\n                        random_state = default_seed,\n                        output_to_file = True):\n    '''\n    This function tests a variety of investment methodologies and their returns. \n    It will run its tests on the loans defined by the test_set element of the data\n    dictionary.\n    \n    It is currently able to test four strategies\n      - random: invest in a random set of loans\n      - ranking: score each loan by probability of default, and only invest\n                 in the \"safest\" loans (i.e., those with the lowest probabilities\n                 of default)\n      - regression: train a single regression model to predict the expected return\n                    of loans in the past. Then, for loans we could invest in, simply\n                    rank them by their expected returns and invest in that order.\n      - two-stage: train two regression models to predict the expected return of\n                   defaulted loans and non-defaulted loans in the training set. Then,\n                   for each potential loan we could invest in, predict the probability\n                   the loan will default, its return if it doesn't default and its\n                   return if it does. Then, calculate a weighted combination of\n                   the latter using the former to find a predicted return. Rank the\n                   loans by this expected return, and invest in that order\n    \n    It expects the following parameters\n      - data_dict: the dictionary containing both training and testing data;\n                   returned by the prepare_data function\n      - classifier: a fitted model object which is returned by the fit_classification function.\n      - regressor: a fitted model object which is returned by the fit_regression function.\n      - strategy: the name of the strategy; one of the three listed above\n      - num_loans: the number of loans to be included in the test portfolio\n      - num_samples: the number of random samples used to compute average return ()   \n      - random_state: the random seed to use when selecting a subset of rows\n      - output_to_file: if the results will be saved to the output file\n      \n    The function returns a dictionary FOR EACH RETURN DEFINITION with the following entries\n      - strategy: the name of the strategy\n      - average return: the return of the strategy based on the testing set\n      - test data: the updated Dataframe of testing data. Useful in the optimization section\n    '''\n    \n    np.random.seed(random_state)\n    \n    # Retrieve the rows that were used to train and test  the\n    # classification model\n    train_set = data_dict['train_set']\n    test_set = data_dict['test_set']\n    \n    col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n    \n    # Create a dataframe for testing, including the score\n    data_test = data.loc[test_set,:]\n    out = {}\n    \n    for ret_col in col_list:    \n    \n        if strategy == 'Random':\n            # Randomize the order of the rows in the datframe\n            data_test = data_test.sample(frac = 1).reset_index(drop = True)\n\n            # Select num_loans to invest in\n            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n\n            # Find the average return for these loans\n            ret_test = np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n\n            # Return\n            out[ret_col] = {'strategy':strategy, 'average return':ret_test}\n\n            # Dump the strategy performance to file\n            if output_to_file:\n                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n\n            continue\n        \n        elif strategy == 'Regression':\n            \n            colname = 'predicted_return_' + ret_col \n\n            data_test[colname] = regressor[ret_col]['predicted_return']\n\n            # Sort the loans by predicted return\n            data_test = data_test.sort_values(by=colname, ascending = False).reset_index(drop = True)\n\n            # Pick num_loans loans\n            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n\n            # Find their return\n            ret_test = np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n\n            # Return\n            out[ret_col] = {'strategy':strategy, 'average return':ret_test, 'test data':data_test}\n\n            # Dump the strategy performance to file\n            if output_to_file:\n                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n\n            continue\n            \n        # Get the predicted scores, if the strategy is not Random or just Regression\n        try:\n            y_pred_score = classifier['y_pred_probs']\n        except:\n            y_pred_score = classifier['y_pred_score']\n\n        data_test['score'] = y_pred_score\n\n\n        if strategy == 'Ranking':\n            # Sort the test data by the score\n            data_test = data_test.sort_values(by='score').reset_index(drop = True)\n\n            # Select num_loans to invest in\n            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n\n            # Find the average return for these loans\n            ret_test = np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n\n            # Return\n            out[ret_col] = {'strategy':strategy, 'average return':ret_test}\n\n            # Dump the strategy performance to file\n            if output_to_file:\n                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n\n            continue\n\n\n        elif strategy == 'Two-stage':\n\n            # Load the predicted returns\n            data_test['predicted_regular_return'] = regressor[ret_col]['predicted_regular_return']\n            data_test['predicted_default_return'] = regressor[ret_col]['predicted_default_return']\n\n            # Compute expectation\n            colname = 'predicted_return_' + ret_col \n            \n            data_test[colname] = ( (1-data_test.score)*data_test.predicted_regular_return + \n                                             data_test.score*data_test.predicted_default_return )\n\n            # Sort the loans by predicted return\n            data_test = data_test.sort_values(by=colname, ascending = False).reset_index(drop = True)\n\n            # Pick num_loans loans\n            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n\n            # Find their return\n            ret_test = np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n\n            # Return\n            out[ret_col] = {'strategy':strategy, 'average return':ret_test, 'test data':data_test}\n\n            # Dump the strategy performance to file\n            if output_to_file:\n                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n\n            continue\n\n        elif strategy == 'Crystal-ball':\n\n            # Sort the loans by realized return\n            data_test = data_test.sort_values(by=ret_col, ascending = False).reset_index(drop = True)\n\n            # Pick num_loans loans\n            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n\n            # Find their return\n            ret_test = np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n\n            # Return\n            out[ret_col] = {'strategy':strategy, 'average return':ret_test}\n\n            # Dump the strategy performance to file\n            if output_to_file:\n                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n\n            continue\n\n        else:\n            return 'Not a valid strategy'\n        \n    return out",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ecbb4a9e7c5a379594e4bd591bc1ad58262b0883"
      },
      "cell_type": "markdown",
      "source": "## Baseline models\nSee how well we do using grade or interest rate only"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b92df4f74fc07133bb83d89d0b8bed9848485c4"
      },
      "cell_type": "code",
      "source": "# Define a function that, given a CVGridSearch object, finds the\n# percentage difference between the best and worst scores\ndef find_score_variation(cv_model):\n    all_scores = cv_model.cv_results_['mean_test_score']\n    return( np.abs((max(all_scores) - min(all_scores))) * 100 / max(all_scores) )\n\n    '''\n    which_min_score = np.argmin(all_scores)\n    \n    all_perc_diff = []\n    \n    try:\n        all_perc_diff.append( np.abs(all_scores[which_min_score - 1] - all_scores[which_min_score])*100 / min(all_scores) )\n    except:\n        pass\n    \n    try:\n        all_perc_diff.append( np.abs(all_scores[which_min_score + 1] - all_scores[which_min_score])*100 / min(all_scores) )\n    except:\n        pass\n    \n    return ( np.mean(all_perc_diff) )\n    '''\n\n# Define a function that checks, given a CVGridSearch object,\n# whether the optimal parameters lie on the edge of the search\n# grid\ndef find_opt_params_on_edge(cv_model):\n    out = False\n    \n    for i in cv_model.param_grid:\n        if cv_model.best_params_[i] in [ cv_model.param_grid[i][0], cv_model.param_grid[i][-1] ]:\n            out = True\n            break\n            \n    return out\n# Create a function to print a line to our output file\n\ndef dump_to_output(key, value):\n    with open(output_file, \"a\") as f:\n        f.write(\",\".join([str(default_seed), key, str(value)]) + \"\\n\")\n        \ndefault_seed = 1\noutput_file = \"output_sample\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08232a44c7095fb5070ccc60f2ec05bffc5ab30d"
      },
      "cell_type": "markdown",
      "source": "### LogisticRegression with Grade"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cce86ce85c4e181fb02c089da60677a9570c2b6"
      },
      "cell_type": "code",
      "source": "from IPython.display import Image\nImage(\"../input/l1andl2/l1andl2.png\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d55fa0fee65cb03ae50460b3a3fc9a537da0d25d"
      },
      "cell_type": "code",
      "source": "data_dict = prepare_data(feature_subset=['grade'])\ngrade_only_logistic = LogisticRegression(penalty = 'l2', C=np.inf, solver='lbfgs')\n\ngrade_only_logistic = fit_classification(grade_only_logistic,data_dict,\n                                         model_name = 'Grade only logistic l2')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1a23b5b709fefb066b7aa819fd6520b4a9b94626"
      },
      "cell_type": "markdown",
      "source": "### LogisticRegression with Interest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c783e047c2e4ba32de1f002130761ac5be160d9"
      },
      "cell_type": "code",
      "source": "data_dict = prepare_data(feature_subset=['int_rate'])\ninterest_only_logistic = LogisticRegression(penalty = 'l2', C=np.inf, solver='lbfgs')\n\ninterest_only_logistic = fit_classification(interest_only_logistic, data_dict, \n                                   model_name = 'Interest rate only logistics l2')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "48529b8f8cc790fa708d62979bfa2e78071a34de"
      },
      "cell_type": "markdown",
      "source": "### Test models without grade or interest rate"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d697878db875f22fc92c750ca458984f012c39f"
      },
      "cell_type": "code",
      "source": "final_features = [i for i in discrete_features + continuous_features if i not in [\"grade\", \"int_rate\", \"installment\"]]\ndata_dict = prepare_data(feature_subset = final_features)\n\nall_features = pd.Series(continuous_features + discrete_features_dummies)\nidx = [i for i, j in enumerate(continuous_features + discrete_features_dummies)\n                                                     if j.split(\"::\")[0] in final_features]\nselected_features = all_features[idx]\nselected_features.reset_index(drop=True,inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f78d5e3fc6858c8028e51909e1e5aeb714b21214"
      },
      "cell_type": "markdown",
      "source": "### Ridge Classifier"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aec2d1a9ce38667552637b40c84df4115b6f737b"
      },
      "cell_type": "code",
      "source": "ridge_classifier = RidgeClassifier()\ncv_parameters = {\"alpha\":np.logspace(-4, 4, num = 10)}\n\nridge_classifier = fit_classification(ridge_classifier, data_dict, \n                             cv_parameters = cv_parameters, model_name = \"Ridge Classifier\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b057d29e2f1345473f90f90c16c6e397dcd3f11"
      },
      "cell_type": "markdown",
      "source": "### Naive Bayes"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b7a8dcbadf806560bd3404f821756fd5adfe73a4"
      },
      "cell_type": "code",
      "source": "gnb = GaussianNB()\ngnb = fit_classification(gnb, data_dict,\n                model_name = \"Gaussian Naive Bayes\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a0c6b2dfab2d2ce7aa5fa0af363e7cb573c931f4"
      },
      "cell_type": "markdown",
      "source": "### ð‘™1penalized logistic regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a287631f7e242cd5fdf63f95b43fa2572a33ad86"
      },
      "cell_type": "code",
      "source": "l1_logistic = LogisticRegression(penalty = 'l1')\ncv_parameters = {\"C\":np.logspace(0, 6, num = 10)}\n\nl1_logistic = fit_classification(l1_logistic, data_dict,\n                        cv_parameters = cv_parameters, model_name = \"l1 Penalized Logistic Regression\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "68f5df87acfb32dd8441e6704c1423ae2d2d1685"
      },
      "cell_type": "markdown",
      "source": "### ð‘™2penalized logistic regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "033c9abd9c7e27821649c17e738de528db0fcc94"
      },
      "cell_type": "code",
      "source": "l2_logistic = LogisticRegression(penalty = 'l2')\ncv_parameters = {\"C\":np.logspace(-4, 4, num = 10)}\n\nl2_logistic = fit_classification(l2_logistic, data_dict,\n                        cv_parameters = cv_parameters, model_name = \"l2 Penalized Logistic Regression\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29aa4fdc35e9b6a4070bc9e447dde507505b85e5"
      },
      "cell_type": "code",
      "source": "## plot top 3 features with the most positive (and negative) weights \ntop_and_bottom_idx = list(np.argsort(l2_logistic['model'].coef_)[0,:3]) + list(np.argsort(l2_logistic['model'].coef_)[0,-3:])\nbplot = pd.Series(l2_logistic['model'].coef_[0,top_and_bottom_idx])\nxticks = selected_features[top_and_bottom_idx]\np1 = bplot.plot(kind='bar',rot=-30,ylim=(-5,10))\np1.set_xticklabels(xticks)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d1ae3d9f9f4fb68ad1d94b100d7f1f4187aec533"
      },
      "cell_type": "markdown",
      "source": "### Decision tree"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "611dbde6bbe8c55f9dddabbcfb71dca400d53776"
      },
      "cell_type": "code",
      "source": "decision_tree = DecisionTreeClassifier()\ncv_parameters = {'min_samples_leaf':[500,600,700,800,900,1000, 1100, 1200, 1300]}\n\ndecision_tree = fit_classification(decision_tree, data_dict, \n                          cv_parameters = cv_parameters, model_name = \"Decision tree\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0939331917236f7948e2373b704513154fbcecc0"
      },
      "cell_type": "code",
      "source": "\n# Visualize the decision tree\n# Zooming-in is allowed by double click\nfrom io import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydotplus #pip install pydotplus\ndot_data = StringIO()\nexport_graphviz(decision_tree['model'], out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a7815327661540862a846343210d7864269ba639"
      },
      "cell_type": "markdown",
      "source": "### Random forest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "039e90f1eb86d125eccf2bfc77bd207c61c4b27b"
      },
      "cell_type": "code",
      "source": "random_forest = RandomForestClassifier()\ncv_parameters = {'min_samples_leaf':[1, 2, 3, 5, 8, 13, 17, 20, 40], 'n_estimators': [35, 60, 80, 100, 150] }\n\nrandom_forest = fit_classification(random_forest, data_dict,\n                                   cv_parameters=cv_parameters, model_name=\"Random forest\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f25e2682de914fd83e43d6666165d89ff7667a4f"
      },
      "cell_type": "code",
      "source": "## Plot top 6 most significant features\ntop_idx = list(np.argsort(random_forest['model'].feature_importances_)[-6:]) \nbplot = pd.Series(random_forest['model'].feature_importances_[top_idx])\nxticks = selected_features[top_idx]\np2 = bplot.plot(kind='bar',rot=-30,ylim=(0,0.2))\np2.set_xticklabels(xticks)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74f3bfd2cbedf0ba81a11b46da8cd1221956fccf"
      },
      "cell_type": "code",
      "source": "## A decision tree trained on the scores of random forest\n\ntrepin_tree = DecisionTreeClassifier(min_samples_leaf = 100, max_depth = 4)\ntrepin_tree.fit(random_forest['y_pred_probs'].reshape(-1,1),data_dict['y_test'])\ndot_data = StringIO()\nexport_graphviz(trepin_tree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "415a3b79d8584e954b68028b91cdfe645cec6da7"
      },
      "cell_type": "markdown",
      "source": "### Bagged trees"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "76b3c6c2dad37201bbbba85b8082e7bc0cd98539"
      },
      "cell_type": "code",
      "source": "bagged_trees = RandomForestClassifier(max_features = 1.0)\ncv_parameters = {'min_samples_leaf':[5, 10], 'n_estimators': [10] }\n\nbagged_trees = fit_classification(bagged_trees, data_dict,\n                                   cv_parameters=cv_parameters, model_name=\"Bagged trees\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "46f2aef874bb14d04403833a27f189e5172d848a"
      },
      "cell_type": "markdown",
      "source": "### Multi-layer perceptron"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff7d706f7dd5d224185089fdf177f0282046da00"
      },
      "cell_type": "code",
      "source": "mlp = MLPClassifier()\ncv_parameters = {'hidden_layer_sizes':[(1), (10), (50), (100), (5, 5), (10, 10)]}\n\nmlp = fit_classification(mlp, data_dict,\n                         cv_parameters = cv_parameters, model_name=\"Multi-Layer Perceptron\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c53610091ada2fb6e299ef15b4e5e83e5cfcd06f"
      },
      "cell_type": "markdown",
      "source": "## Test regression models\n### Lasso-Lars regressor"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e41970b259e7e5ce4f6cff0107576cbbd8a5992"
      },
      "cell_type": "code",
      "source": "# First, trying LASSO penalized regression with a variety of parameters,\n# it becomes clear a simple regression is best\n\ncv_parameters = {'alpha': np.logspace(-8, -1, num = 8) }\n\nreg_lasso = fit_regression(linear_model.LassoLars(), data_dict,\n               cv_parameters = cv_parameters, separate = False, model_name = \"Lasso\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d7748535dec165c7ffbe718877241d13f34d462a"
      },
      "cell_type": "markdown",
      "source": "### Ridge regressor"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "314d0e9faa41c3a2ab4652094757eae8f140e793"
      },
      "cell_type": "code",
      "source": "cv_parameters = {'alpha': np.logspace(-8, -1, num = 8) }\n\nreg_ridge = fit_regression(linear_model.Ridge(), data_dict,\n               cv_parameters = cv_parameters, separate = False, model_name = \"Ridge\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08a31183ec5f0b67a761e4ebe85ea9c37c6bb7ae"
      },
      "cell_type": "markdown",
      "source": "### Ordinary least squares"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "635c2f5e049901270d076436fd9db056e610c147"
      },
      "cell_type": "code",
      "source": "reg_linear = fit_regression(linear_model.LinearRegression(), data_dict,\n               separate = False, model_name = \"Linear Regression\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "17d023a783bcb7c39aec382354d4d1716563a577"
      },
      "cell_type": "markdown",
      "source": "### Multi-layer perceptron regressor"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a20c901c67d8a61e2eeb94c9ff16c05921769e05"
      },
      "cell_type": "code",
      "source": "cv_parameters = { 'alpha':[0.001, 0.01, 0.1, 1, 10, 100],\n                  'hidden_layer_sizes':[(1), (10), (50), (100), (200), (5, 5), (10, 10)] }\n\nreg_mlp = fit_regression(MLPRegressor(), data_dict,\n               cv_parameters = cv_parameters, separate = False, model_name = \"Multi-Layer Perceptron\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "80b67ee1d9bc391cb1ac0fec19f46c7768a3a199"
      },
      "cell_type": "markdown",
      "source": "### Random forest regressor"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e8dd07af6e75b80de61dc01158311efe8dce93d"
      },
      "cell_type": "code",
      "source": "cv_parameters = {'min_samples_leaf':[75, 100, 200, 300, 400],\n                 'n_estimators': [35, 45, 55, 65, 80, 90, 100] }\n\nreg_rf = fit_regression(RandomForestRegressor(), data_dict,\n               cv_parameters = cv_parameters, separate = False, model_name = \"Random forest regressor\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f62fa87a75c6d5b41784a2ba54bf34b2667994a"
      },
      "cell_type": "markdown",
      "source": "### Learning curve of classifiers\nStart with 25 training points and double the training size every time (This cell will take at least 30 mins to run)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da717c16c9d4d44709ad8c88b8ef266791ef007f"
      },
      "cell_type": "code",
      "source": "test_sizes = np.array([25 * 2**k for k in range(10)])\nnum_rows = len(test_sizes)\nT = 10\ndf_learning_logistic = pd.DataFrame(data=np.zeros((num_rows,T)),columns=list(range(T)))\ndf_learning_rf = pd.DataFrame(data=np.zeros((num_rows,T)),columns=list(range(T)))\n\ndf_learning_logistic.index = test_sizes\ndf_learning_rf.index = test_sizes\n\nfor t in range(T):\n    \n    # just to get test data; training size is irrelavent here\n    data_dict_t = prepare_data(n_samples_train=10000,n_samples_test=10000,\n                                   feature_subset=final_features, random_state=t)\n    X_test_t = data_dict_t['X_test']\n    y_test_t = data_dict_t['y_test']\n    \n    for size in test_sizes:\n        \n        # get training data based on the specified size\n        data_dict_t = prepare_data(n_samples_train=int(size),n_samples_test=10000,\n                                   feature_subset=final_features, random_state=t)\n        \n        l2_logistic_t = LogisticRegression(penalty = 'l2')\n        cv_parameters = {\"C\":np.logspace(-4, 4, num = 10)}\n\n        l2_logistic_t = fit_classification(l2_logistic_t, data_dict_t,\n                            cv_parameters = cv_parameters, model_name = \"l2 Penalized Logistic Regression\",\n                                      print_to_screen = False, output_to_file = False)\n\n        random_forest_t = RandomForestClassifier()\n        cv_parameters = {'min_samples_leaf':[1, 2, 3, 5, 8, 13, 17, 20], 'n_estimators': [35, 60, 80, 100] }\n\n        random_forest_t = fit_classification(random_forest_t, data_dict_t,\n                                       cv_parameters=cv_parameters, model_name=\"Random forest\",\n                                             print_to_screen = False, output_to_file = False)\n\n        df_learning_logistic.loc[size,t] = roc_auc_score(y_test_t, l2_logistic_t['model'].predict_proba(X_test_t)[:,1])\n        df_learning_rf.loc[size,t] = roc_auc_score(y_test_t, random_forest_t['model'].predict_proba(X_test_t)[:,1])\n\n\ndf_learning = pd.DataFrame(data=np.zeros((num_rows,2)),columns=['l2 logistic', 'random forest'])\ndf_learning.index = test_sizes\ndf_learning.loc[:,'l2 logistic'] = np.mean(df_learning_logistic.iloc[:,:11],axis=1)\ndf_learning.loc[:,'random forest'] = np.mean(df_learning_rf.iloc[:,:11], axis=1)\ndf_learning.plot()\nplt.ylabel('Testing AUC')\nplt.xlabel('Training Size')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2808357174bf73d7a38c2febabb81b4fad1a8c69"
      },
      "cell_type": "markdown",
      "source": "## Time stability test\n### On the whole time period"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1b5c58e54daae24dfe7c17b5e8a02e8fef7f2de"
      },
      "cell_type": "code",
      "source": "data_dict_test = prepare_data(n_samples_train = 9000, n_samples_test = 7000, feature_subset = final_features)\ncv_parameters = {'min_samples_leaf':[1, 2, 3, 5, 8, 13, 17, 20, 40], 'n_estimators': [5, 10, 15, 20, 25] }\n\nfit_classification(RandomForestClassifier(), data_dict_test,\n                   cv_parameters = cv_parameters, model_name = \"Random forest\", output_to_file = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1d95a355a8f87245ca2eb737f2d6d48aad73f554"
      },
      "cell_type": "markdown",
      "source": "### Train and test on the first 2 years"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90d9bceaabfbdea767a8fac2a17e84e2ed6a4e1b"
      },
      "cell_type": "code",
      "source": "np.sum(data.issue_d < datetime.date(2011,1,1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d25e7878149f430362a6cb173b0a71b9145527e"
      },
      "cell_type": "code",
      "source": "start_date = datetime.date(2009,1,1)\nend_date = datetime.date(2010,12,1)\n\ndata_dict_test = prepare_data(date_range_train = (start_date, end_date), date_range_test = (start_date, end_date),\n                         n_samples_train = 9000, n_samples_test = 7000, feature_subset = final_features)\n\ncv_parameters = {'min_samples_leaf':[1, 2, 3, 5, 8, 13, 17, 20, 40], 'n_estimators': [5, 10, 15, 20, 25] }\n\nfit_classification(RandomForestClassifier(), data_dict_test,\n                   cv_parameters = cv_parameters, model_name = \"Random forest\", output_to_file = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "71254a8c08ee3a8c317808061e68e3a67343ac6b"
      },
      "cell_type": "markdown",
      "source": "Train on the first two years and test on the last year"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7958adf712066fc6efaffb6af4d872095d8a9072"
      },
      "cell_type": "code",
      "source": "start_date_train = datetime.date(2009,1,1)\nend_date_train = datetime.date(2010,12,1)\nstart_date_test = datetime.date(2017,1,1)\nend_date_test = datetime.date(2017,12,1)\n\ndata_dict_test = prepare_data(date_range_train = (start_date_train, end_date_train), \n                         date_range_test = (start_date_test, end_date_test),\n                         n_samples_train = 9000, n_samples_test = 7000, feature_subset = final_features)\n\ncv_parameters = {'min_samples_leaf':[1, 2, 3, 5, 8, 13, 17, 20, 40], 'n_estimators': [5, 10, 15, 20, 25] }\n\nfit_classification(RandomForestClassifier(), data_dict_test,\n                   cv_parameters = cv_parameters, model_name = \"Random forest\", output_to_file = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de8668bc6945b15b372f9888c83eb74147b87a2d"
      },
      "cell_type": "markdown",
      "source": "## Test investment strategies\nNow we test several investment strategies using the learning models above"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf33e5179e58cf54da38775cced4e39f1e933baf"
      },
      "cell_type": "code",
      "source": "col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\ntest_strategy = 'Random'\n\nprint('strategy:',test_strategy)   \nstrat_random = test_investments(data_dict,strategy = test_strategy, \n                            num_loans = 1000, output_to_file = False, random_state = 1)\nfor ret_col in col_list:\n    print(ret_col + ': ' + str(strat_random[ret_col]['average return']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1bc503d146fbc0283fa4300a7e00dea484a3631"
      },
      "cell_type": "code",
      "source": "test_strategy = 'Ranking'\n\nprint('strategy:',test_strategy)\nstrat_rank = test_investments(data_dict, classifier=random_forest, strategy = test_strategy, \n                        num_loans = 1000, output_to_file = False)\n\nfor ret_col in col_list:\n    print(ret_col + ': ' + str(strat_rank[ret_col]['average return']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d516ad2fa5b216422e298d45c6bbf46c454b259"
      },
      "cell_type": "code",
      "source": "test_strategy = 'Regression'\n\nprint('strategy:',test_strategy)\nstrat_reg = test_investments(data_dict, regressor=reg_rf, strategy = test_strategy, \n                        num_loans = 1000)\nfor ret_col in col_list:\n    print(ret_col + ': ' + str(strat_reg[ret_col]['average return']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43784c8d25d998cb9f45d8c7d81922130343f9fa"
      },
      "cell_type": "code",
      "source": "test_strategy = 'Two-stage'\n\n## For the two-stage strategy we need to fit a new RF regressor with separate = True\ncv_parameters = {'min_samples_leaf':[25, 50, 75, 100, 200, 300, 400],\n                 'n_estimators': [35, 45, 55, 65, 80, 90, 100] }\n\nreg_rf_separate = fit_regression(RandomForestRegressor(), data_dict,\n                       cv_parameters = cv_parameters, separate = True, model_name = \"Random forest regressor\", \n                       print_to_screen = False, output_to_file = False)\n\nprint('strategy:',test_strategy)\ntwo_stage = test_investments(data_dict, classifier = random_forest, regressor = reg_rf_separate, \n                             strategy = test_strategy, num_loans = 1000)\n\nfor ret_col in col_list:\n    print(ret_col + ': ' + str(two_stage[ret_col]['average return']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9357a6d41e8cb3af64ffa3ac87fd9f4a49d6e45d"
      },
      "cell_type": "markdown",
      "source": "## Sensitivity test of portfolio size"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0680b9678975a2c152dd0193de4158c47c3177b4"
      },
      "cell_type": "code",
      "source": "result_sensitivity = []\n\n## Vary the portfolio size from 1,000 to 10,000\nfor num_loans in list(range(1000,10000,1000)):\n\n    reg_0 = test_investments(data_dict, regressor = reg_rf_separate, classifier = random_forest, \n                            strategy = 'Two-stage', num_loans = num_loans)\n    result_sensitivity.append(reg_0['ret_PESS']['average return'])\n    \nresult_sensitivity = np.array(result_sensitivity) * 100\nsns.pointplot(np.array(list(range(1000,10000,1000))),result_sensitivity)\nsns.despine()\nplt.ylabel('Investment Return (%)',size = 14)\nplt.xlabel('Portfolio Size',size = 14)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "506d874278bdaae8ca0955ad04c38a8704d94cbe"
      },
      "cell_type": "markdown",
      "source": "## Optimization: an example\nIn this section, we implement three different optimization models. To illustrate and compare these models we will only use the M1-PESS definition and the predicted returns from the previously tested two-stage strategy.\n### Directly maximize total profit"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e44f06cfa8025af039f5a3a3509b5b40ec6a0c0"
      },
      "cell_type": "code",
      "source": "import itertools\n!pip install pyomo\nfrom pyomo.core import *\nfrom pyomo.environ import *\nfrom pyomo.pysp.annotations import(PySP_ConstraintStageAnnotation,PySP_StochasticRHSAnnotation,StochasticConstraintBodyAnnotation)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "825b91132e71c2387343ebcbfc8ddd4add873ecb"
      },
      "cell_type": "code",
      "source": "# In this section we implement two versions of optimization problem\n# The first one is simply maximizing the total expected profit\nret_col = 'ret_PESS'\ntest_pool = two_stage[ret_col]['test data']\nnum_loans = 1000\n\n# create the model\nm = Model('model_maximizing_profit')\nVar = {}\n\n# add the binary variable to the model\nfor i in range(test_pool.shape[0]):\n        Var[i] = m.addVar(vtype=GRB.BINARY)\n\n# add the number of loans constraint\nm.addConstr(num_loans == quicksum(Var[i] for i in range(test_pool.shape[0])))\n\n# set the objective to maximize total profit\nm.setObjective(quicksum(Var[i]*test_pool['predicted_return_'+ret_col].iloc[i]*test_pool.funded_amnt.iloc[i]\n                        for i in range(test_pool.shape[0])), GRB.MAXIMIZE)\n\nm.setParam('OutputFlag',False)\nm.optimize()\n\n# save only the selected loans and compute the average return\ntest_pool['selection'] = np.array(m.X)\npf_0 = test_pool[test_pool.selection > 0]\n\nprint('return:',np.dot(pf_0[ret_col],pf_0.funded_amnt)/np.sum(pf_0.funded_amnt))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53cfb0ff3f019cbff8a160b1833496447856f40f"
      },
      "cell_type": "markdown",
      "source": "### Maximize profit with budget constraint"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a0744f4c6e0bf6298089eb8832cbea8b3d1aa91b"
      },
      "cell_type": "code",
      "source": "m = Model('model_maximizing_profit_constrained')\nVar = {}\nBudget = 10*1000000\n\nfor i in range(test_pool.shape[0]):\n    Var[i] = m.addVar(vtype=GRB.BINARY)\n\n# add the budget constraint\nm.addConstr(Budget >= quicksum(Var[i] * test_pool.loan_amnt.iloc[i]\n                       for i in range(test_pool.shape[0])))\n\nm.addConstr(num_loans >= quicksum(Var[i] for i in range(test_pool.shape[0])))\n\n# add a minimum number of loans constraint\nm.addConstr(900 <= quicksum(Var[i] for i in range(test_pool.shape[0])))\n\nm.setObjective(quicksum(Var[i]*test_pool['predicted_return_'+ret_col].iloc[i]*test_pool.loan_amnt.iloc[i]\n                        for i in range(test_pool.shape[0])), GRB.MAXIMIZE)\n\nm.setParam('OutputFlag',False)\nm.optimize()\n\ntest_pool['selection'] = np.array(m.X)\npf_0 = test_pool[test_pool.selection > 0]\n\nprint('return:',np.dot(pf_0[ret_col],pf_0.funded_amnt)/np.sum(pf_0.funded_amnt))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "053205ccaeefbcfa88624bb7abcee0cdd8b5a9a2"
      },
      "cell_type": "markdown",
      "source": "### Maximize profit with risk-return tradeoff"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "593fdeee95915e0f3f0d4f81920f268989e07d49"
      },
      "cell_type": "code",
      "source": "## First we need to train a clustering model to estimate the variance of return\nn_clusters = 5\n\ntrain_set = data_dict['train_set']\ndata_train = data.loc[train_set,:]\n\n# Create a dataframe for testing, including the score\ndata_test = two_stage[ret_col]['test data']\n\nkmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data_dict['X_train'])\ndata_train['clusID'] = kmeans.predict(data_dict['X_train'])\ndata_test['clusID'] = kmeans.predict(data_dict['X_test'])\ndata_test['volatility'] = 0\n\nfor idx in range(n_clusters):\n    std_clus = np.std(data_train[ret_col][data_train.clusID == idx])\n    data_test.volatility[data_test.clusID == idx] = std_clus\n\n## Specify the parameters of the optimization model\n# beta: penalty factor on the risk\nbeta = 0.009\nBudget = 10.7*1000000\n\nm = Model('model_volatility_constrained')\n\nfor i in range(test_pool.shape[0]):\n    Var[i] = m.addVar(vtype=GRB.BINARY)\n\n# add the budget constraint\nm.addConstr(Budget >= quicksum(Var[i] * test_pool.loan_amnt.iloc[i]\n                       for i in range(test_pool.shape[0])))\n\nm.addConstr(num_loans >= quicksum(Var[i] for i in range(test_pool.shape[0])))\nm.addConstr(900 <= quicksum(Var[i] for i in range(test_pool.shape[0])))\n\n# the expected return is adjusted based on the risk\nm.setObjective(quicksum(Var[i]*(test_pool['predicted_return_'+  ret_col].iloc[i] -\n                                beta * test_pool.volatility.iloc[i])\n                                *test_pool.loan_amnt.iloc[i]\n                                for i in range(test_pool.shape[0])), GRB.MAXIMIZE)\n\nm.setParam('OutputFlag',False)\nm.optimize()\n\ntest_pool['selection'] = np.array(m.X)\npf_0 = test_pool[test_pool.selection > 0]\n\nprint('return:',np.dot(pf_0[ret_col],pf_0.funded_amnt)/np.sum(pf_0.funded_amnt))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9ecfdcd56340819588ce9fdb0a02d01d6bd15ce"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abd66a56754d6618203e31e3e6c31f44bd18aae9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2d3c5e56e3d318844328fe3bbd019d523d657a5"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "747f6858d33e4ccecd4769db63e19e648b586294"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}